{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 12:11:49.519375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 12:11:49.519873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 12:11:49.520302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data from the dataset fetch_20newsgroups.\n",
    "\n",
    "To this project, was used just 10 of the 20 categories, more than that could take much longer to train the algorithms\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    " 'comp.graphics',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5578 news for training\n",
      "There are 3714 news for testing\n"
     ]
    }
   ],
   "source": [
    "train_data = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "train_y = train_data.target\n",
    "\n",
    "test_data = fetch_20newsgroups(\n",
    "    subset='test', \n",
    "    categories=categories, \n",
    "    shuffle=True, \n",
    "    random_state=42\n",
    ")\n",
    "test_y = test_data.target\n",
    "\n",
    "print(f\"There are {len(train_data.data)} news for training\")\n",
    "print(f\"There are {len(test_data.data)} news for testing\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in it's raw form, to don't input too much desnecessary data, it need to be cleaned.\n",
    "\n",
    "So, remove some features of the news, such as the from and some not alphabetical chars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_data(data):\n",
    "    \"\"\"\n",
    "    function to run in each string of the data set and apply the cleaning \n",
    "    \n",
    "    Args:\n",
    "        temp (string): variable to hold the values on each string while cleaning the dataset\n",
    "\n",
    "    Returns:\n",
    "        data (list of string) - lists containing the data cleaned\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        temp = data[i]\n",
    "        temp = temp.split('\\n')\n",
    "        #removing the 'from' headline\n",
    "        temp  = temp[1:]\n",
    "\n",
    "        #removing empty indexes\n",
    "        temp = [x for x in temp if x != '']\n",
    "\n",
    "        #removing not alphabetical chars\n",
    "        temp = [re.sub(r'/^[\\w&.\\-]+$/',' ',i) for i in temp]\n",
    "    \n",
    "        data[i] = ' '.join(temp)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cleaning function and separating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = adjust_data(train_data.data)\n",
    "test_x = adjust_data(test_data.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(News, labels):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and validation sets\n",
    "    split in 80% to training and 20% to validation\n",
    "    \n",
    "    Args:\n",
    "        News (list of string): lower-cased News\n",
    "        labels (list of string): list of labels\n",
    "    \n",
    "    Returns:\n",
    "        train_x, val_x, train_y, val_y - lists containing the data splits\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Compute the number of News that will be used for training (should be an integer)\n",
    "    training_size = int(len(News)*0.8)\n",
    "\n",
    "    # Split the News and labels into train/validation splits\n",
    "    train_x = News[0:training_size]\n",
    "    train_y = labels[0:training_size]\n",
    "\n",
    "    val_x = News[training_size:]\n",
    "    val_y = labels[training_size:]\n",
    "    \n",
    "    return train_x, val_x, train_y, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_val_split(train_x, train_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenazing and padding data\n",
    "\n",
    "To input the data in the algorithm it's needed to change the vacabulary data to a numeric data, so, keras tokenizer can help with that\n",
    "\n",
    "after the tokenization, it's necessary a padding to every new have the same len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict =  {\n",
    "    'NUM_WORDS': 1000,\n",
    "    'PADDING' : 'post',\n",
    "    'OOV_TOKEN' : '<OOV>'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 56190 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=token_dict['NUM_WORDS'], \n",
    "                        oov_token=token_dict['OOV_TOKEN'])\n",
    "\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Vocabulary contains {len(word_index)} words\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    # Pad the sequences using the correct padding and maxlen\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=maxlen,\n",
    "                                    padding=padding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4e57ba4b097f0246d49f53384aceda62719be828b7c5c4e8ea79d36cd20af42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
